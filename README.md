# Replication Package for "_Reproducibility, Detecting Coding Errors and Robustness Analysis: Comparing the Performance of Humans, Cyborgs, and Machines with Limited Human Assistance_"

## Table of Content
- [Abstract](#Abstract)
- [Replication Workflows in **R** and **Stata**](#Replication-Workflows-in-R-and-Stata)
- [Folder Structure](#Folder-Structure)
- [Software and Tools](#Software-and-Tools)
- [Instructions to Run the **Stata** Workflow](#Instructions-to-Run-the-Stata-Workflow)
- [Instructions to Run the **R** Workflow](#Instructions-to-Run-the-R-Workflow)
- [Contact Information](#Contact-Information)

## Abstract
This article evaluates the effectiveness of varying levels of human and artificial intelligence (AI) integration in reproducibility assessments of quantitative social science research. In study I which was conducted in 2024, we computationally reproduced quantitative results from published articles in the social sciences with 288 researchers, randomly assigned to 103 teams across three groups -- human-only teams, AIassisted teams and teams whose task was to minimally guide an AI to conduct reproducibility checks (the “AI-led” approach). In study II which was conducted in 2025, 95 researchers participated again and were randomly assigned to 34 teams. Findings reveal that when working independently, human teams matched the reproducibility success rates of teams using AI assistance, while both groups substantially outperformed AI-led approaches (with human teams achieving 52 percentage points higher success rates than AI-led teams, p < 0.001). Human teams were particularly effective at identifying serious problems in the analysis: on average they found significantly more major errors compared to both AI-assisted teams (0.7 more errors per team, p = 0.017) and AI-led teams (1.1 more errors per team, p < 0.001). AI-assisted teams demonstrated an advantage over more automated approaches, detecting 0.4 more major errors per team than AI-led teams (p = 0.029), though still significantly fewer than human-only teams. Finally, both human and AI-assisted teams significantly outperformed AI-led approaches in both proposing (25 percentage points difference, p = 0.017) and implementing (33 percentage points difference, p = 0.005) comprehensive robustness checks.

These results underscore both the strengths and current limitations of LLM-based assistance -- specifically OpenAI’s GPT-4/4o and 4.5/o3 as deployed in study I and II, respectively -- in research reproduction. While these findings are bounded by the capabilities of the specific models used, the experimental conditions, and the time frame of study, they provide an important benchmark for what was feasible at the time. They also suggest that, in this context, these models -- despite their sophistication -- were not yet able to replace or significantly enhance human judgment in complex empirical workflows. While newer versions of AI tools narrowed this gap, as of now, meaningful human involvement remains essential for accurate and reliable reproducibility assessments in quantitative social science.

## Replication Workflows in **R** and Stata
This project includes two fully independent and equivalent replication workflows: one implemented in Stata and the other in R. Both workflows use the same underlying datasets and reproduce all main tables and figures reported in the manuscript. Users may choose either language depending on their preference or software availability.

The [code folder](https://github.com/atyho/AI-Replication-Games/tree/main/code) contains parallel scripts in R (with the .R extension) and Stata (with the .do extension). All outputs generated by both workflows are identical in substance, and all results can be independently verified using either language.

While the Stata scripts use .dta data and the R scripts use .rds, both datasets are derived from the same raw source files provided in the [data folder](https://github.com/atyho/AI-Replication-Games/tree/main/data).

## Folder Structure

## Software and Tools

## Instruction to Run the **Stata** Workflow

## Instruction to Run the **R** Workflow

## Contact Information
For more details about the replication process or to address specific issues, please refer to the corresponding author information provided in the manuscript. Alternatively, consult the supplementary materials for additional insights.
